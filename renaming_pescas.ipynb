{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> <font color = \"bisque\"> Tareas:</font> </u>\n",
    "## <font color = \"bisque\"> * Implementar procesos de renombre en TOPICO PESCAS</font> \n",
    "\n",
    "---\n",
    "\n",
    "<b> <font color = \"LightSalmon\"> MODO DE TRABAJO PROCESO DE RENOMBRE - v2 </font> </b>\n",
    "\n",
    "Voy a trabajar sobre argendata/data que esté hasta ahora, en un fork propio.\n",
    "\n",
    "<b> Sobre argendata/data lo que voy a hacer es tomar los .csv  originales y hacer lo siguiente (dependiendo el caso), generando .csv actualizados:</b>\n",
    "\n",
    "\n",
    "   - Si un dataset tiene un código que esté incluido en los geocodigos del geonomenclador, se nomencla con la string de name_long\n",
    "\n",
    "   - Si un dataset tiene un código que NO este incluido en los geocodigos y tiene una string que matchee sin ambigüedad en desc_fundar, entonces se usa el código que está en el geonomenclador (y se guarda la asociación del codigo encontrado con el codigo usado)\n",
    "\n",
    "   - Si un dataset tiene un código que NO este incluido en los geocodigos y tiene una string que NO matchea sin ambigüedad en desc_fundar, se genera un código nuevo (a revisión para ser desambiguado con posibles matches)\n",
    "\n",
    "   - Si un dataset tiene una string que matchee sin ambigüedad en desc_fundar entonces se usa el código que está en el geonomenclador para esa string\n",
    "\n",
    "   - Si un dataset tiene una string que NO matchee sin ambigüedad en desc_fundar, entonces se genera un código nuevo (a revisión para ser desambiguado con posibles matches)\n",
    "\n",
    "\n",
    "<b> ¿Cómo busco las columnas de interés para esta iteración? </b>\n",
    "- X% (por ejemplo 80%) del contenido de columnas de interés de un .csv de un tópico es parte de alguna de las columnas del geoneomenclador \n",
    "- Por el momento solo código que esté en el geonomenclador en español\n",
    "\n",
    "Observaciones: \n",
    "- PARA ESTE LABURO ARMÉ MI PROPIO BLOQUE DE ENCODERs, DELIMITERs\n",
    "- Para esta versión de la tarea, en principio, solo consideraría contenido en español\n",
    "\n",
    "<b> <font color = \"LightSalmon\"> Productos: \n",
    "- .csv argendata con agregado de columnas para implementar renombres correspondientes\n",
    "- .csv con lo que mencioné antes con .csv de argendata que presentarían problemas </font> </b>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <u> <font color = \"orangered\"> Cómo abrir cada .csv considerando su encoding: </font> </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"gold\"> Cada .csv de Argendata tiene su propio encodings y delimiters </font>. \n",
    "\n",
    "No todos los .csv que contempla Argendata tienen el mismo enconding y delimiter. Por eso antes de comenzar a trabajar voy a armar el siguiente input para la tarea: dataframe que contenga 4 columnas que sean 'TOPICO', 'archivo_csv', 'encoding' y delimiter'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TOPICO                                        archivo_csv    encoding  \\\n",
      "0   PESCAS        31_consumo_per_capita_pescado_anio_pais.csv       utf-8   \n",
      "1   PESCAS  16_valor_cantidad_precio_exportacion_pesquero.csv       ascii   \n",
      "2   PESCAS  01_produccion_pesquera_captura_y_acuicola_por_...       utf-8   \n",
      "3   PESCAS                                22_pbg_pesquero.csv       utf-8   \n",
      "4   PESCAS             04_desembarque_especie_ultimo_anio.csv       ascii   \n",
      "5   PESCAS                      14_evolucion_pib_pesquero.csv       ascii   \n",
      "6   PESCAS  07_acuicultura_produccion_por_pais_ultimo_anio...       utf-8   \n",
      "7   PESCAS  18_valor_precio_cantidad_expo_por_especie_anio...       ascii   \n",
      "8   PESCAS                          11_total_desembarques.csv       ascii   \n",
      "9   PESCAS  09_composicion_consumo_carne_animal_por_tipo_y...       utf-8   \n",
      "10  PESCAS         29_produccion_vs_consumo_pesca_arg_evo.csv       ascii   \n",
      "11  PESCAS              21_acuicultura_produccion_arg_evo.csv       ascii   \n",
      "12  PESCAS                              27_vab_expo_share.csv       ascii   \n",
      "13  PESCAS    03_composicion_exportaciones_pesca_producto.csv       utf-8   \n",
      "14  PESCAS          20_captura_merluza_hubbsi_vs_cmp_anio.csv       ascii   \n",
      "15  PESCAS           30_share_global_expo_pesca_pais_anio.csv       utf-8   \n",
      "16  PESCAS                       28_consumo_pescado_decil.csv       ascii   \n",
      "17  PESCAS          02_complejos_exportadores_ultimo_anio.csv       utf-8   \n",
      "18  PESCAS     15_participacion_complejo_pesquero_en_expo.csv       ascii   \n",
      "19  PESCAS                        12_capturas_grupos_anio.csv       utf-8   \n",
      "20  PESCAS  26_produccion_pesquera_captura_y_acuicola_shar...       ascii   \n",
      "21  PESCAS       05_expo_pesquera_por_especie_ultimo_anio.csv       ascii   \n",
      "22  PESCAS                        13_captura_especie_anio.csv       ascii   \n",
      "23  PESCAS  08_consumo_pescado_mariscos_per_capita_pais_ul...       utf-8   \n",
      "24  PESCAS    23_share_acuicola_total_pesca_por_pais_anio.csv       utf-8   \n",
      "25  PESCAS  25_produccion_pesquera_captura_y_acuicola_mund...       ascii   \n",
      "26  PESCAS                06_desembarques_puertos_totales.csv  ISO-8859-1   \n",
      "\n",
      "   delimiter  \n",
      "0          ,  \n",
      "1          ,  \n",
      "2          ,  \n",
      "3          ,  \n",
      "4          ,  \n",
      "5          ,  \n",
      "6          ,  \n",
      "7          ,  \n",
      "8          ,  \n",
      "9          ,  \n",
      "10         ,  \n",
      "11         ,  \n",
      "12         ,  \n",
      "13         ,  \n",
      "14         ,  \n",
      "15         ,  \n",
      "16         ,  \n",
      "17         ,  \n",
      "18         ,  \n",
      "19         ,  \n",
      "20         ,  \n",
      "21         ,  \n",
      "22         ,  \n",
      "23         ,  \n",
      "24         ,  \n",
      "25         ,  \n",
      "26         ,  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import chardet\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# ruta de tabajo donde estan los topicos con sus csv \n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/pesca_renaming/data_argendata' # '/home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata' \n",
    "\n",
    "# lista para recolectar los datos\n",
    "data = []\n",
    "\n",
    "# tamanio del fragmento a leer (en este caso 100000 bytes)\n",
    "sample_size = 100000\n",
    "\n",
    "for topico in os.listdir(base_dir): # ejemplo SEBACO\n",
    "    topico_path = os.path.join(base_dir, topico) #ejemplo /home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata/SEBACO\n",
    "\n",
    "    # ignorar archivos sueltos como LICENSE y README.md\n",
    "    if not os.path.isdir(topico_path):\n",
    "        continue\n",
    "\n",
    "    for archivo in os.listdir(topico_path):\n",
    "        if archivo.endswith('.csv'):\n",
    "            archivo_path = os.path.join(topico_path, archivo)\n",
    "\n",
    "            # leer un fragmento del .csv para análisis\n",
    "            with open(archivo_path, 'rb') as f:\n",
    "                raw_sample = f.read(sample_size)\n",
    "\n",
    "            # detectar encoding\n",
    "            encoding_detected = 'utf-8'  # por defecto\n",
    "            try:\n",
    "                encoding_info = chardet.detect(raw_sample)\n",
    "                if encoding_info and encoding_info['encoding']:\n",
    "                    encoding_detected = encoding_info['encoding']\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # decodificar el fragmento\n",
    "            try:\n",
    "                sample = raw_sample.decode(encoding_detected, errors = 'replace')\n",
    "            except:\n",
    "                sample = raw_sample.decode('utf-8', errors = 'replace') \n",
    "\n",
    "            # detectar delimitador\n",
    "            delimiter_detected = ','\n",
    "            try:\n",
    "                sniffer = csv.Sniffer()\n",
    "                dialect = sniffer.sniff(sample)\n",
    "                delimiter_detected = dialect.delimiter\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # agregar al resultado\n",
    "            data.append({\n",
    "                'TOPICO': topico,\n",
    "                'archivo_csv': archivo,\n",
    "                'encoding': encoding_detected,\n",
    "                'delimiter': delimiter_detected\n",
    "            })\n",
    "\n",
    "\n",
    "df_resultado = pd.DataFrame(data)\n",
    "df_codificacion = df_resultado\n",
    "\n",
    "print(df_codificacion)\n",
    "\n",
    "# guardar como csv el archivo csvTopicoArgendata_encoding_and_delimiters\n",
    "# path_df_encoding_delimiters = '/home/lucia/Desktop/Fundar/Argendata_renaming/data/'\n",
    "df_codificacion.to_csv('/home/capuccino/Desktop/TrabajoFundar/pesca_renaming/' + 'PESCA_encoding_delimiters.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <u> <font color = \"orangered\">  Tareas concretas del proceso de renaming </font> </u>\n",
    "\n",
    "<b> <font color = \"gold\"> Sobre argendata/data lo que voy a hacer es tomar los .csv  originales y hacer lo siguiente (dependiendo el caso), generando .csv actualizados: </font> </b>\n",
    "\n",
    "   - si un dataset tiene un código que esté incluido en los geocodigos del geonomenclador, se nomencla con la string de name_long (si es que existe), y si no es así se nomencla con la string de desc_fundar correspondiente\n",
    "\n",
    "   - Si un dataset tiene un código que NO este incluido en los geocodigos y tiene una string que matchee sin ambigüedad en desc_fundar, entonces se usa el código que está en el geonomenclador (y se guarda la asociación del codigo encontrado con el codigo usado)\n",
    "\n",
    "   - Si un dataset tiene un código que NO este incluido en los geocodigos y tiene una string que NO matchea sin ambigüedad en desc_fundar, se genera un código nuevo (a revisión para ser desambiguado con posibles matches)\n",
    "\n",
    "   - Si un dataset tiene una string que matchee sin ambigüedad en desc_fundar entonces se usa el código que está en el geonomenclador para esa string\n",
    "\n",
    "   - Si un dataset tiene una string que NO matchee sin ambigüedad en desc_fundar, entonces se genera un código nuevo (a revisión para ser desambiguado con posibles matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font color = \"gold\"> Para hacer esto, empecemos por: </font> </b>\n",
    "\n",
    "1. abriría el archivo según su encoding y delimiter dado por df_codificacion \n",
    "\n",
    "2. detectaría si cada .csv de tiene una columna que se corresponda a la columna geocodigo del csv geonomenclador según la siguiente condición: fijarse si, al menos, el 80% de sus filas se encuentra contenida en la columna geocodigo del geonomenclador\n",
    "\n",
    "3. detectaría si cada .csv de tiene una columna que se corresponda a la columna desc_fundar del csv geonomenclador según la siguiente condición: fijarse si, al menos, el 80% de sus filas se encuentra contenida en la columna desc_fundar del geonomenclador\n",
    "\n",
    "4. esta información guardarla en un dataframe llamado columnscsv_Geocodigo_descFundar, con formato de 4 columnas que sean 'TOPICO', 'archivo_csv', 'columna_Geocodigo', 'columna_DescFundar'. Donde sea 'columna_Geocodigo' es el nombre de la columna de dicho cvs que cumple la condición 60% (80%, X%) geocodigo del geonomenclador y 'descFundar' es el nombre de la columna de dicho csv que cumple la condición 60% (80%, X%) desc_fundar del geonomenclador. \n",
    "\n",
    "A la hora de hacer la comparación del 60%:\n",
    "- De la columna analizada la columna, eliminar nulos\n",
    "- normalizar espacios, mayúsculas, tildes, para que casos como los siguientes no rompan el X%: \"NEA \" y \"NEA\" o \"pampeana\" y \"Pampeana\" o \"Córdoba\" y \"Córdoba\" se reconozcan como idénticos\n",
    "\n",
    "<b> <font color = \"bisque\"> Esto resulta en un df con: TOPICO | archivo_csv | columna_Geocodigo | columna_DescFundar </font> </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOPICO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "archivo_csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "columna_Geocodigo",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "columna_DescFundar",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7d9bc8a7-a21d-484d-b1de-cd8c654dcd01",
       "rows": [
        [
         "0",
         "SALING",
         "ISA_composicion-ipcf_it4.csv",
         null,
         null
        ],
        [
         "1",
         "SALING",
         "ISA_tipo_empleo_i1.csv",
         null,
         null
        ],
        [
         "2",
         "SALING",
         "ISA_salarios_region_i2.csv",
         null,
         null
        ],
        [
         "3",
         "SALING",
         "ISA_salario_real_i1.csv",
         null,
         null
        ],
        [
         "4",
         "SALING",
         "ISA_ipcf_it1.csv",
         null,
         null
        ],
        [
         "5",
         "SALING",
         "ISA_regiones-ipcf_it1.csv",
         null,
         null
        ],
        [
         "6",
         "SALING",
         "ISA_salario_real_i3.csv",
         null,
         null
        ],
        [
         "7",
         "SALING",
         "salario_real_ppa2017_ceped.csv",
         "iso3",
         null
        ],
        [
         "8",
         "SALING",
         "ISA_composicion-ipcf_it3.csv",
         null,
         null
        ],
        [
         "9",
         "SALING",
         "ISA_salarios_mundo_i2.csv",
         null,
         null
        ],
        [
         "10",
         "SALING",
         "ISA_salarios_mundo_i1.csv",
         "pais",
         null
        ],
        [
         "11",
         "SALING",
         "ISA_salarios_region_i1.csv",
         null,
         null
        ],
        [
         "12",
         "SALING",
         "ISA_composicion-ipcf_it5.csv",
         null,
         null
        ],
        [
         "13",
         "SALING",
         "ISA_regiones-ipcf_it2.csv",
         null,
         null
        ],
        [
         "14",
         "SALING",
         "ISA_composicion-ipcf_it1.csv",
         null,
         null
        ],
        [
         "15",
         "SALING",
         "ISA_tipo_empleo_i3.csv",
         null,
         "pais"
        ],
        [
         "16",
         "SALING",
         "ISA_edad_genero_i2.csv",
         null,
         null
        ],
        [
         "17",
         "SALING",
         "ISA_edad_genero_i1.csv",
         null,
         null
        ],
        [
         "18",
         "SALING",
         "ISA_ipcf-LAC_it2.csv",
         "country_code",
         null
        ],
        [
         "19",
         "SALING",
         "ISA_edad_genero_i3.csv",
         null,
         null
        ],
        [
         "20",
         "SALING",
         "salario_real_base1970.csv",
         null,
         null
        ],
        [
         "21",
         "SALING",
         "ISA_salario_real_i2.csv",
         null,
         null
        ],
        [
         "22",
         "SALING",
         "ISA_composicion-ipcf_it2.csv",
         null,
         null
        ],
        [
         "23",
         "SALING",
         "ISA_tipo_empleo_i2.csv",
         null,
         null
        ],
        [
         "24",
         "SALING",
         "prima_formalidad_argentina.csv",
         null,
         null
        ],
        [
         "25",
         "SALING",
         "ISA_ipcf-LAC_it1.csv",
         "country_code",
         null
        ],
        [
         "26",
         "INFDES",
         "anios_educacion_genero_por_pais.csv",
         "iso3",
         "pais"
        ],
        [
         "27",
         "INFDES",
         "tasa_desempleo_eph_niveled.csv",
         null,
         null
        ],
        [
         "28",
         "INFDES",
         "tasa_informalidad_provincia_definicion_productiva_legal.csv",
         null,
         "prov_desc"
        ],
        [
         "29",
         "INFDES",
         "satisfaccion_vida_desempleo_pais.csv",
         "iso3",
         "pais_desc"
        ],
        [
         "30",
         "INFDES",
         "tasa_informalidad_legal_latam.csv",
         "iso3",
         "pais"
        ],
        [
         "31",
         "INFDES",
         "tasa_informalidad_legal_por_edad_sexo.csv",
         null,
         null
        ],
        [
         "32",
         "INFDES",
         "tasa_informalidad_productive_legal_ultimo_anio_latam.csv",
         "iso3",
         "pais"
        ],
        [
         "33",
         "INFDES",
         "tasa_desempleo_ephtu_sexo_edad.csv",
         null,
         null
        ],
        [
         "34",
         "INFDES",
         "tasa_informalidad_argentina_tipo_anio.csv",
         null,
         null
        ],
        [
         "35",
         "INFDES",
         "tasa_formalidad_productiva_pib_per_capita.csv",
         "iso3",
         "pais"
        ],
        [
         "36",
         "INFDES",
         "tasa_desempleo_sexo_paises_modelada.csv",
         "iso3",
         "pais_desc"
        ],
        [
         "37",
         "INFDES",
         "tasa_desempleo_arg_mundial_modelada.csv",
         "iso3",
         "pais_desc"
        ],
        [
         "38",
         "INFDES",
         "tasa_informalidad_argentina_genero.csv",
         null,
         null
        ],
        [
         "39",
         "INFDES",
         "composicion_empleo_ultimo_anio_latam.csv",
         "iso3",
         "pais"
        ],
        [
         "40",
         "INFDES",
         "brecha_tasa_informalidad_genero_latam_2000_2021.csv",
         "iso3",
         "pais"
        ],
        [
         "41",
         "PRECIO",
         "6_comparacion_inflacion_mediana_latam_altosing_1980_2022.csv",
         null,
         null
        ],
        [
         "42",
         "PRECIO",
         "1_composicion_ipc_argentina.csv",
         null,
         null
        ],
        [
         "43",
         "PRECIO",
         "7_comparacion_inflacion_mediana_argentina_latam_1992_2022.csv",
         null,
         null
        ],
        [
         "44",
         "PRECIO",
         "4_tasa_de_inflacion_anual_paises_promedio_2007_2022.csv",
         "iso3",
         null
        ],
        [
         "45",
         "PRECIO",
         "ponderadores_engho_evolucion.csv",
         null,
         null
        ],
        [
         "46",
         "PRECIO",
         "11_tasa_de_inflacion_mensual_argentina_feb1989_jun1990.csv",
         null,
         null
        ],
        [
         "47",
         "PRECIO",
         "precios_relativos_empalmada.csv",
         null,
         null
        ],
        [
         "48",
         "PRECIO",
         "tasa_de_inflacion_anual_argentina_1935_2022_valores_positivos.csv",
         null,
         null
        ],
        [
         "49",
         "PRECIO",
         "precios_relativos_caba_empalmada.csv",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 337
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPICO</th>\n",
       "      <th>archivo_csv</th>\n",
       "      <th>columna_Geocodigo</th>\n",
       "      <th>columna_DescFundar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SALING</td>\n",
       "      <td>ISA_composicion-ipcf_it4.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SALING</td>\n",
       "      <td>ISA_tipo_empleo_i1.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SALING</td>\n",
       "      <td>ISA_salarios_region_i2.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SALING</td>\n",
       "      <td>ISA_salario_real_i1.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SALING</td>\n",
       "      <td>ISA_ipcf_it1.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_actividad_por_pais_anio.csv</td>\n",
       "      <td>iso3</td>\n",
       "      <td>iso3_desc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_participacion_censos.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tiempo_social_trabajo_sexo_niveleducativo.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tasa_empleo_por_franja_etaria_anio_provincia.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>prov_desc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>MERTRA</td>\n",
       "      <td>tipo_trabajo_no_rem_sexo.csv</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TOPICO                                       archivo_csv  \\\n",
       "0    SALING                      ISA_composicion-ipcf_it4.csv   \n",
       "1    SALING                            ISA_tipo_empleo_i1.csv   \n",
       "2    SALING                        ISA_salarios_region_i2.csv   \n",
       "3    SALING                           ISA_salario_real_i1.csv   \n",
       "4    SALING                                  ISA_ipcf_it1.csv   \n",
       "..      ...                                               ...   \n",
       "332  MERTRA                  tasa_actividad_por_pais_anio.csv   \n",
       "333  MERTRA                     tasa_participacion_censos.csv   \n",
       "334  MERTRA     tiempo_social_trabajo_sexo_niveleducativo.csv   \n",
       "335  MERTRA  tasa_empleo_por_franja_etaria_anio_provincia.csv   \n",
       "336  MERTRA                      tipo_trabajo_no_rem_sexo.csv   \n",
       "\n",
       "    columna_Geocodigo columna_DescFundar  \n",
       "0                None               None  \n",
       "1                None               None  \n",
       "2                None               None  \n",
       "3                None               None  \n",
       "4                None               None  \n",
       "..                ...                ...  \n",
       "332              iso3          iso3_desc  \n",
       "333              None               None  \n",
       "334              None               None  \n",
       "335              None          prov_desc  \n",
       "336              None               None  \n",
       "\n",
       "[337 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion para eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# rutas y dataframes necesarios\n",
    "\n",
    "base_dir =  '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata' # '/home/lucia/Desktop/Fundar/Argendata_renaming/data/data_argendata' # \n",
    "# df_codificacion hecho\n",
    "df_geonomenclador = pd.read_csv('/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/input' + '/geonomenclador_LuloTest.csv')\n",
    "\n",
    "# sets normalizados de las columnas de interes del geonomenclador: geocodigo y desc_fundar \n",
    "# estoy probndo usar set para practicar, si es necesario paso a lista\n",
    "set_geocod = set(\n",
    "    df_geonomenclador['geocodigo']\n",
    "      .dropna().astype(str) #elimino nulos\n",
    "      .str.strip() # quitar espacios al inicio/final\n",
    "      .str.upper() # pasar todo a mayusculas\n",
    "      .apply(sin_tildes) # sacar tildes\n",
    ")\n",
    "set_desc = set(\n",
    "    df_geonomenclador['desc_fundar']\n",
    "      .dropna().astype(str)\n",
    "      .str.strip()\n",
    "      .str.upper()\n",
    "      .apply(sin_tildes)\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "# los topicos (directorios) ignorando los archivos LICENSE y README.md\n",
    "topics = []\n",
    "for d in os.listdir(base_dir):\n",
    "    item_path = os.path.join(base_dir, d)\n",
    "    \n",
    "    if os.path.isdir(item_path): # verifico que sea un directorio\n",
    "        \n",
    "        if d != 'LICENSE' and d != 'README.md': # no incluyo LICENSE y README.md\n",
    "            topics.append(d)\n",
    "\n",
    "for topico in topics:\n",
    "    topic_dir = os.path.join(base_dir, topico)\n",
    "    # listo los .csv de este tópico\n",
    "    csv_files = [f for f in os.listdir(topic_dir) if f.endswith('.csv')]\n",
    "    for archivo in csv_files:\n",
    "        # uso el encoding y delimiter de df_codificacion\n",
    "        row = df_codificacion[\n",
    "            (df_codificacion['TOPICO'] == topico) &\n",
    "            (df_codificacion['archivo_csv'] == archivo)\n",
    "        ]\n",
    "        if row.empty:\n",
    "            print(f'No hay info de encoding/delimiter para {topico}/{archivo}') # quiero probar no estar pifiando\n",
    "            continue\n",
    "        encoding  = row.iloc[0]['encoding']\n",
    "        delimiter = row.iloc[0]['delimiter']\n",
    "        path_csv  = os.path.join(topic_dir, archivo)\n",
    "\n",
    "        # leo csv con el encoding delimiter\n",
    "        try:\n",
    "            df_csv = pd.read_csv(path_csv, encoding = encoding, delimiter = delimiter)\n",
    "        except Exception as e: \n",
    "            print(f'Error leyendo {path_csv}: {e}') # imprimo el error\n",
    "            continue\n",
    "\n",
    "        col_geocod = None\n",
    "        col_desc   = None\n",
    "\n",
    "        # analizar cada columna del .csv\n",
    "        for col in df_csv.columns:\n",
    "            serie = (\n",
    "                df_csv[col]\n",
    "                  .dropna().astype(str)\n",
    "                  .str.strip()\n",
    "                  .str.upper()\n",
    "                  .apply(sin_tildes)\n",
    "            )\n",
    "            n = len(serie)\n",
    "            if n == 0:\n",
    "                continue\n",
    "\n",
    "            match_geo  = serie.isin(set_geocod).sum() / n\n",
    "            match_desc = serie.isin(set_desc).sum()   / n\n",
    "\n",
    "            if match_geo  >= 0.6 and col_geocod is None:\n",
    "                col_geocod = col\n",
    "            if match_desc >= 0.6 and col_desc   is None:\n",
    "                col_desc = col\n",
    "\n",
    "        results.append({\n",
    "            'TOPICO': topico,\n",
    "            'archivo_csv': archivo,\n",
    "            'columna_Geocodigo': col_geocod,\n",
    "            'columna_DescFundar': col_desc\n",
    "        })\n",
    "\n",
    "# creo df de interes\n",
    "columnscsv_Geocodigo_descFundar = pd.DataFrame(results)\n",
    "\n",
    "columnscsv_Geocodigo_descFundar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "geocodigo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "desc_fundar",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name_long",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "name_short",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Unnamed: 4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Unnamed: 5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Unnamed: 6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Unnamed: 7",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e0fe61f9-06a9-414b-b2ef-6549fcb4a79b",
       "rows": [
        [
         "0",
         "ABW",
         "Aruba",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "AFE",
         "África Oriental y Meridional",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "AFG",
         "Afganistán",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "AFW",
         "África Occidental y Central",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "AGO",
         "Angola",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "5",
         "AIA",
         "Anguila",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "ALA",
         "Islas Åland",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "ALB",
         "Albania",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "8",
         "AND",
         "Andorra",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "ANT",
         "Antillas Holandesas",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "10",
         "ARB",
         "Mundo Árabe",
         "Mundo árabe",
         "Mundo árabe",
         null,
         null,
         null,
         " "
        ],
        [
         "11",
         "ARE",
         "Emiratos Árabes Unidos",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "12",
         "ARG",
         "Argentina",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "13",
         "ARM",
         "Armenia",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "14",
         "ASM",
         "Samoa Americana",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "15",
         "ATA",
         "Antártida",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "16",
         "ATF",
         "Territorio de las Tierras Australes Francesas",
         "Tierras Australes Francesas",
         "Tierras Australes Francesas",
         null,
         null,
         null,
         null
        ],
        [
         "17",
         "ATG",
         "Antigua y Barbuda",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "18",
         "AUS",
         "Australia",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "19",
         "AUT",
         "Austria",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "20",
         "AZE",
         "Azerbaiyán",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "21",
         "BAT",
         "Territorio Antártico Británico",
         "Antártida Británica",
         "Antártida Británica",
         null,
         null,
         null,
         null
        ],
        [
         "22",
         "BDI",
         "Burundi",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "23",
         "BEL",
         "Bélgica",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "24",
         "BEN",
         "Benin",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "25",
         "BES",
         "Bonaire, San Eustaquio y Saba",
         "Bonaire",
         "Bonaire",
         null,
         null,
         null,
         null
        ],
        [
         "26",
         "BFA",
         "Burkina Faso",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "27",
         "BGD",
         "Bangladesh",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "28",
         "BGR",
         "Bulgaria",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "29",
         "BHR",
         "Bahrein",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "30",
         "BHS",
         "Bahamas",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "31",
         "BIH",
         "Bosnia y Herzegovina",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "32",
         "BLM",
         "San Barthélemy",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "33",
         "BLR",
         "Belarús",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "34",
         "BLX",
         "Bélgica-Luxemburgo",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "35",
         "BLZ",
         "Belice",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "36",
         "BMU",
         "Bermuda",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "37",
         "BOL",
         "Bolivia",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "38",
         "BRA",
         "Brasil",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "39",
         "BRB",
         "Barbados",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "40",
         "BRN",
         "Brunei Darussalam",
         "Brunei",
         "Brunei",
         null,
         null,
         null,
         null
        ],
        [
         "41",
         "BTN",
         "Bhután",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "42",
         "BVT",
         "Isla Bouvet",
         "Bouvet",
         "Bouvet",
         null,
         null,
         null,
         null
        ],
        [
         "43",
         "BWA",
         "Botswana",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "44",
         "CAF",
         "República Centroafricana",
         "Rep. Centroafricana",
         "Rep. Centroafricana",
         null,
         null,
         null,
         null
        ],
        [
         "45",
         "CAN",
         "Canadá",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "46",
         "CCK",
         "Islas Cocos (Keeling)",
         "Islas Cocos",
         "Islas Cocos",
         null,
         null,
         null,
         null
        ],
        [
         "47",
         "CEB",
         "Europa Central y Los Países Bálticos",
         "Europa Central y Bálticos",
         "Europa Central y Bálticos",
         null,
         null,
         null,
         null
        ],
        [
         "48",
         "CEM",
         "Comunidad Económica y Monetaria de Africa Central (CEMAC)",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "49",
         "CHE",
         "Suiza",
         null,
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 1024
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geocodigo</th>\n",
       "      <th>desc_fundar</th>\n",
       "      <th>name_long</th>\n",
       "      <th>name_short</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABW</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFE</td>\n",
       "      <td>África Oriental y Meridional</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Afganistán</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFW</td>\n",
       "      <td>África Occidental y Central</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGO</td>\n",
       "      <td>Angola</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>DESHUM_AHDI.WOF</td>\n",
       "      <td>Ramificaciones de Occidente (AHDI)</td>\n",
       "      <td>Ramificaciones de Occidente</td>\n",
       "      <td>Ramificaciones de Occidente</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>NIR</td>\n",
       "      <td>Irlanda del Norte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>CAMCLI_IA</td>\n",
       "      <td>Aviación internacional</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>F_ESTPRO</td>\n",
       "      <td>África (OECD)</td>\n",
       "      <td>África</td>\n",
       "      <td>África</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>NAFTA_ESTPRO</td>\n",
       "      <td>Países miembros del NAFTA (OECD)</td>\n",
       "      <td>Países miembros del NAFTA</td>\n",
       "      <td>Países miembros del NAFTA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1024 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            geocodigo                         desc_fundar  \\\n",
       "0                 ABW                               Aruba   \n",
       "1                 AFE        África Oriental y Meridional   \n",
       "2                 AFG                          Afganistán   \n",
       "3                 AFW         África Occidental y Central   \n",
       "4                 AGO                              Angola   \n",
       "...               ...                                 ...   \n",
       "1019  DESHUM_AHDI.WOF  Ramificaciones de Occidente (AHDI)   \n",
       "1020              NIR                   Irlanda del Norte   \n",
       "1021        CAMCLI_IA              Aviación internacional   \n",
       "1022         F_ESTPRO                       África (OECD)   \n",
       "1023     NAFTA_ESTPRO    Países miembros del NAFTA (OECD)   \n",
       "\n",
       "                        name_long                   name_short  Unnamed: 4  \\\n",
       "0                             NaN                          NaN         NaN   \n",
       "1                             NaN                          NaN         NaN   \n",
       "2                             NaN                          NaN         NaN   \n",
       "3                             NaN                          NaN         NaN   \n",
       "4                             NaN                          NaN         NaN   \n",
       "...                           ...                          ...         ...   \n",
       "1019  Ramificaciones de Occidente  Ramificaciones de Occidente         NaN   \n",
       "1020                          NaN                          NaN         NaN   \n",
       "1021                          NaN                          NaN         NaN   \n",
       "1022                       África                       África         NaN   \n",
       "1023    Países miembros del NAFTA    Países miembros del NAFTA         NaN   \n",
       "\n",
       "      Unnamed: 5  Unnamed: 6 Unnamed: 7  \n",
       "0            NaN         NaN        NaN  \n",
       "1            NaN         NaN        NaN  \n",
       "2            NaN         NaN        NaN  \n",
       "3            NaN         NaN        NaN  \n",
       "4            NaN         NaN        NaN  \n",
       "...          ...         ...        ...  \n",
       "1019         NaN         NaN        NaN  \n",
       "1020         NaN         NaN        NaN  \n",
       "1021         NaN         NaN        NaN  \n",
       "1022         NaN         NaN        NaN  \n",
       "1023         NaN         NaN        NaN  \n",
       "\n",
       "[1024 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geonomenclador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font color = \"gold\"> Continuamos con lo que enumero a continuación para cumplir las condiciones detalladas enteriormente (los 5 bullets de más arriba): </font> </b>\n",
    "\n",
    "Ya generamos el df columnscsv_Geocodigo_descFundar con: TOPICO | archivo_csv | columna_Geocodigo | columna_DescFundar, \n",
    "Donde 'columna_Geocodigo' es el nombre de la columna de dicho cvs que cumple la condición X% geocodigo del geonomenclador y 'descFundar' es el nombre de la columna de dicho csv que cumple la condición X% desc_fundar del geonomenclador.\n",
    "\n",
    "<b> <font color = \"gold\">Por cada tópico (carpeta) y cada csv dentro de este, busco generar una nueva carpeta de cada tópico y dentro de cada carpeta guardar una copia de sus respectivos csvs pero agregandoles a los mismos dos columnas: una que sea 'geocodigoFundar' y otra que sea 'geonombreFundar'.</font> </b>\n",
    "\n",
    "Si para un csv de un topico tanto columna_Geocodigo como columna_DescFundar es NA, ya sabremos que el csv copia se guardará con NA tanto en las columnas 'geocodigoFundar' como 'geonombreFundar'.\n",
    "\n",
    "Siendo que columnscsv_Geocodigo_descFundar ya tiene por cada tópico y csv, si dicho csv tiene una columna 'columna_Geocodigo' que sería de geocodigo (condicion X% de columna geocodigo del df_geonomenclador) y otra columna 'columna_DescFundar' (condicion del 80 % de la columna desc_fundar del df_geonomenclador), voy a usarcolumnscsv_Geocodigo_descFundar para completar 'geocodigoFundar' y 'geonombreFundar'.\n",
    "\n",
    "Para rellenar las columnas 'geocodigoFundar' y 'geonombreFundar', hay que tener las siguientes consideraciones:\n",
    "\n",
    "- Si un dataset (cada csv original de cada topico) tiene una columna asociada a geocodigo segun columnscsv_Geocodigo_descFundar, el contenido de la columna geocodigoFundar del csv copia tendrá el contenido de la columna asociada a geocodigo segun columnscsv_Geocodigo_descFundar. La columna geonombreFundar del csv copia llenará cada una de sus filas acorde al name_long (si es que existe), y si no es así al string de desc_fundar correspondiente a ese geocodigo del geonomenclador.\n",
    "  \n",
    "- En el caso de que un dataset tiene un código que NO esté incluido en los geocodigos pero tiene una string que matchee sin ambigüedad en desc_fundar, entonces se usa el código que está en geocódigo en el geonomenclador (esto se registrará en un dataframe df_problemas con las columnas TOPICO | csv | problema, donde TOPICO es el topico con el que se corresponde, csv el csv evaluado y en problema, en este caso poner: \"dataset tiene el codigo [completar el codigo], que NO esta incluido en los geocodigos pero tiene la string [completar la string] que matchea sin ambiguedad en desc_fundar)\". En [completar el codigo] y [completar la string], rellena lo correspondiente. Entonces, por cada problema guardar la string y el código correspondiente. Esto da a lugar que un mismo csv de un tópico tenga varios problemas a registrar.\n",
    "\n",
    "- Si un dataset tiene un código que NO este incluido en los geocodigos y tiene una string que NO matchea sin ambigüedad en desc_fundar, se genera un código nuevo. Guardar este caso en el dataframe llamado df_problemas con las columnas TOPICO | csv | problema, donde TOPICO es el topico con el que se corresponde, csv el csv evaluado y en problema poner: \"dataset tiene el codigo [completar el codigo] que NO esta incluido en los geocodigos y tiene la string [completar la string] que NO matchea sin ambiguedad en desc_fundar\". En [completar el codigo] y [completar la string], rellena lo correspondiente. Entonces, por cada problema guardar la string y el código correspondiente. Esto da a lugar que un mismo csv de un tópico tenga varios problemas a registrar.\n",
    "\n",
    "- Si un dataset tiene una string que matchee sin ambigüedad en desc_fundar del geonomenclador entonces se usa el código que está en el geonomenclador para esa string.\n",
    "\n",
    "- Si un dataset tiene una string que NO matchee sin ambigüedad en desc_fundar, entonces guardar este caso en el dataframe llamado df_problemas con las columnas TOPICO | csv | problema, donde TOPICO es el topico con el que se corresponde, csv el csv evaluado y en problema poner: \"Para la string [completar la string] generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    iso3        iso3_desc_fundar       valor geocodigoFundar geonombreFundar\n",
      "0    AFG              Afganistán    28480.02            <NA>            <NA>\n",
      "1    AGO                  Angola    55384.74            <NA>            <NA>\n",
      "2    ALB                 Albania    13250.00            <NA>            <NA>\n",
      "3    ARE  Emiratos Árabes Unidos    46494.00            <NA>            <NA>\n",
      "4    ARG               Argentina  2294101.41            <NA>            <NA>\n",
      "..   ...                     ...         ...             ...             ...\n",
      "190  WSM                   Samoa      403.33            <NA>            <NA>\n",
      "191  YEM                   Yemen   198878.58            <NA>            <NA>\n",
      "192  ZAF               Sudáfrica  1910000.00            <NA>            <NA>\n",
      "193  ZMB                  Zambia    50533.34            <NA>            <NA>\n",
      "194  ZWE                Zimbabwe   113000.00            <NA>            <NA>\n",
      "\n",
      "[195 rows x 5 columns]\n",
      "iso3\n",
      "    iso3        iso3_desc_fundar       valor geocodigoFundar  \\\n",
      "0    AFG              Afganistán    28480.02             AFG   \n",
      "1    AGO                  Angola    55384.74             AGO   \n",
      "2    ALB                 Albania    13250.00             ALB   \n",
      "3    ARE  Emiratos Árabes Unidos    46494.00             ARE   \n",
      "4    ARG               Argentina  2294101.41             ARG   \n",
      "..   ...                     ...         ...             ...   \n",
      "190  WSM                   Samoa      403.33             WSM   \n",
      "191  YEM                   Yemen   198878.58             YEM   \n",
      "192  ZAF               Sudáfrica  1910000.00             ZAF   \n",
      "193  ZMB                  Zambia    50533.34             ZMB   \n",
      "194  ZWE                Zimbabwe   113000.00             ZWE   \n",
      "\n",
      "            geonombreFundar  \n",
      "0                Afganistán  \n",
      "1                    Angola  \n",
      "2                   Albania  \n",
      "3    Emiratos Árabes Unidos  \n",
      "4                 Argentina  \n",
      "..                      ...  \n",
      "190                   Samoa  \n",
      "191                   Yemen  \n",
      "192               Sudáfrica  \n",
      "193                  Zambia  \n",
      "194                Zimbabwe  \n",
      "\n",
      "[195 rows x 5 columns]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE UN CSV DE UN TOPICO ARGENDATA QUE ESPERO QUE NO TENGA PROBLEMAS: SE GENEROOOOO!!!! :D\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'AGROPE'  \n",
    "ARCHIVO   = 'produccion_global_carne_aviar_2021.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    prov_cod            prov_desc  tasa_empleo_18_65_mujeres  \\\n",
      "0          2                 CABA                   0.755180   \n",
      "1          6         Buenos Aires                   0.625285   \n",
      "2         10            Catamarca                   0.590940   \n",
      "3         14              Córdoba                   0.585850   \n",
      "4         18           Corrientes                   0.482550   \n",
      "5         22                Chaco                   0.505072   \n",
      "6         26               Chubut                   0.594072   \n",
      "7         30           Entre Ríos                   0.569709   \n",
      "8         34              Formosa                   0.421378   \n",
      "9         38                Jujuy                   0.623202   \n",
      "10        42             La Pampa                   0.623782   \n",
      "11        46             La Rioja                   0.585750   \n",
      "12        50              Mendoza                   0.588962   \n",
      "13        54             Misiones                   0.611542   \n",
      "14        58              Neuquén                   0.599055   \n",
      "15        62            Río Negro                   0.542795   \n",
      "16        66                Salta                   0.595649   \n",
      "17        70             San Juan                   0.515784   \n",
      "18        74             San Luis                   0.622475   \n",
      "19        78           Santa Cruz                   0.619858   \n",
      "20        82             Santa Fe                   0.614329   \n",
      "21        86  Santiago del Estero                   0.552190   \n",
      "22        90              Tucumán                   0.598329   \n",
      "23        94     Tierra del Fuego                   0.608843   \n",
      "\n",
      "    prop_usa_lavarropas geocodigoFundar geonombreFundar  \n",
      "0              0.766963            <NA>            <NA>  \n",
      "1              0.692271            <NA>            <NA>  \n",
      "2              0.565825            <NA>            <NA>  \n",
      "3              0.723312            <NA>            <NA>  \n",
      "4              0.506944            <NA>            <NA>  \n",
      "5              0.430127            <NA>            <NA>  \n",
      "6              0.814129            <NA>            <NA>  \n",
      "7              0.631294            <NA>            <NA>  \n",
      "8              0.415525            <NA>            <NA>  \n",
      "9              0.615616            <NA>            <NA>  \n",
      "10             0.797228            <NA>            <NA>  \n",
      "11             0.701220            <NA>            <NA>  \n",
      "12             0.843693            <NA>            <NA>  \n",
      "13             0.548459            <NA>            <NA>  \n",
      "14             0.793524            <NA>            <NA>  \n",
      "15             0.734564            <NA>            <NA>  \n",
      "16             0.547347            <NA>            <NA>  \n",
      "17             0.745884            <NA>            <NA>  \n",
      "18             0.789810            <NA>            <NA>  \n",
      "19             0.842580            <NA>            <NA>  \n",
      "20             0.666890            <NA>            <NA>  \n",
      "21             0.328093            <NA>            <NA>  \n",
      "22             0.437296            <NA>            <NA>  \n",
      "23             0.916818            <NA>            <NA>  \n",
      "None\n",
      "    prov_cod            prov_desc  tasa_empleo_18_65_mujeres  \\\n",
      "0          2                 CABA                   0.755180   \n",
      "1          6         Buenos Aires                   0.625285   \n",
      "2         10            Catamarca                   0.590940   \n",
      "3         14              Córdoba                   0.585850   \n",
      "4         18           Corrientes                   0.482550   \n",
      "5         22                Chaco                   0.505072   \n",
      "6         26               Chubut                   0.594072   \n",
      "7         30           Entre Ríos                   0.569709   \n",
      "8         34              Formosa                   0.421378   \n",
      "9         38                Jujuy                   0.623202   \n",
      "10        42             La Pampa                   0.623782   \n",
      "11        46             La Rioja                   0.585750   \n",
      "12        50              Mendoza                   0.588962   \n",
      "13        54             Misiones                   0.611542   \n",
      "14        58              Neuquén                   0.599055   \n",
      "15        62            Río Negro                   0.542795   \n",
      "16        66                Salta                   0.595649   \n",
      "17        70             San Juan                   0.515784   \n",
      "18        74             San Luis                   0.622475   \n",
      "19        78           Santa Cruz                   0.619858   \n",
      "20        82             Santa Fe                   0.614329   \n",
      "21        86  Santiago del Estero                   0.552190   \n",
      "22        90              Tucumán                   0.598329   \n",
      "23        94     Tierra del Fuego                   0.608843   \n",
      "\n",
      "    prop_usa_lavarropas geocodigoFundar      geonombreFundar  \n",
      "0              0.766963            <NA>                 <NA>  \n",
      "1              0.692271            AR-B         Buenos Aires  \n",
      "2              0.565825            AR-K            Catamarca  \n",
      "3              0.723312            AR-X              Córdoba  \n",
      "4              0.506944            AR-W           Corrientes  \n",
      "5              0.430127            AR-H                Chaco  \n",
      "6              0.814129            AR-U               Chubut  \n",
      "7              0.631294            AR-E           Entre Ríos  \n",
      "8              0.415525            AR-P              Formosa  \n",
      "9              0.615616            AR-Y                Jujuy  \n",
      "10             0.797228            AR-L             La Pampa  \n",
      "11             0.701220            AR-F             La Rioja  \n",
      "12             0.843693            AR-M              Mendoza  \n",
      "13             0.548459            AR-N             Misiones  \n",
      "14             0.793524            AR-Q              Neuquén  \n",
      "15             0.734564            AR-R            Río Negro  \n",
      "16             0.547347            AR-A                Salta  \n",
      "17             0.745884            AR-J             San Juan  \n",
      "18             0.789810            AR-D             San Luis  \n",
      "19             0.842580            AR-Z           Santa Cruz  \n",
      "20             0.666890            AR-S             Santa Fe  \n",
      "21             0.328093            AR-G  Santiago del Estero  \n",
      "22             0.437296            AR-T              Tucumán  \n",
      "23             0.916818            AR-V     Tierra del Fuego  \n",
      "   TOPICO                                       csv  \\\n",
      "0  MERTRA  lavarropas_tasa_empleo_fem_provincia.csv   \n",
      "\n",
      "                                            problema  \n",
      "0  Para la string 'CABA' generar un codigo nuevo ...  \n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'MERTRA'  \n",
    "# ARCHIVO   = 'lavarropas_tasa_empleo_fem_provincia.csv'\n",
    "# Para la string 'CABA' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'MERTRA'  \n",
    "ARCHIVO   = 'lavarropas_tasa_empleo_fem_provincia.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      iso3                    iso3_desc  anio  \\\n",
      "0      AFE  Africa Eastern and Southern  2023   \n",
      "1      AFE  Africa Eastern and Southern  2022   \n",
      "2      AFE  Africa Eastern and Southern  2021   \n",
      "3      AFE  Africa Eastern and Southern  2020   \n",
      "4      AFE  Africa Eastern and Southern  2019   \n",
      "...    ...                          ...   ...   \n",
      "16699  ZWE                     Zimbabwe  1964   \n",
      "16700  ZWE                     Zimbabwe  1963   \n",
      "16701  ZWE                     Zimbabwe  1962   \n",
      "16702  ZWE                     Zimbabwe  1961   \n",
      "16703  ZWE                     Zimbabwe  1960   \n",
      "\n",
      "       ratio_tasa_actividad_mujer_varon nivel_agregacion geocodigoFundar  \\\n",
      "0                                   NaN       agregacion            <NA>   \n",
      "1                                   NaN       agregacion            <NA>   \n",
      "2                                   NaN       agregacion            <NA>   \n",
      "3                                   NaN       agregacion            <NA>   \n",
      "4                                   NaN       agregacion            <NA>   \n",
      "...                                 ...              ...             ...   \n",
      "16699                               NaN             pais            <NA>   \n",
      "16700                               NaN             pais            <NA>   \n",
      "16701                               NaN             pais            <NA>   \n",
      "16702                               NaN             pais            <NA>   \n",
      "16703                               NaN             pais            <NA>   \n",
      "\n",
      "      geonombreFundar  \n",
      "0                <NA>  \n",
      "1                <NA>  \n",
      "2                <NA>  \n",
      "3                <NA>  \n",
      "4                <NA>  \n",
      "...               ...  \n",
      "16699            <NA>  \n",
      "16700            <NA>  \n",
      "16701            <NA>  \n",
      "16702            <NA>  \n",
      "16703            <NA>  \n",
      "\n",
      "[16704 rows x 7 columns]\n",
      "iso3\n",
      "      iso3                    iso3_desc  anio  \\\n",
      "0      AFE  Africa Eastern and Southern  2023   \n",
      "1      AFE  Africa Eastern and Southern  2022   \n",
      "2      AFE  Africa Eastern and Southern  2021   \n",
      "3      AFE  Africa Eastern and Southern  2020   \n",
      "4      AFE  Africa Eastern and Southern  2019   \n",
      "...    ...                          ...   ...   \n",
      "16699  ZWE                     Zimbabwe  1964   \n",
      "16700  ZWE                     Zimbabwe  1963   \n",
      "16701  ZWE                     Zimbabwe  1962   \n",
      "16702  ZWE                     Zimbabwe  1961   \n",
      "16703  ZWE                     Zimbabwe  1960   \n",
      "\n",
      "       ratio_tasa_actividad_mujer_varon nivel_agregacion geocodigoFundar  \\\n",
      "0                                   NaN       agregacion             AFE   \n",
      "1                                   NaN       agregacion             AFE   \n",
      "2                                   NaN       agregacion             AFE   \n",
      "3                                   NaN       agregacion             AFE   \n",
      "4                                   NaN       agregacion             AFE   \n",
      "...                                 ...              ...             ...   \n",
      "16699                               NaN             pais             ZWE   \n",
      "16700                               NaN             pais             ZWE   \n",
      "16701                               NaN             pais             ZWE   \n",
      "16702                               NaN             pais             ZWE   \n",
      "16703                               NaN             pais             ZWE   \n",
      "\n",
      "                    geonombreFundar  \n",
      "0      África Oriental y Meridional  \n",
      "1      África Oriental y Meridional  \n",
      "2      África Oriental y Meridional  \n",
      "3      África Oriental y Meridional  \n",
      "4      África Oriental y Meridional  \n",
      "...                             ...  \n",
      "16699                      Zimbabwe  \n",
      "16700                      Zimbabwe  \n",
      "16701                      Zimbabwe  \n",
      "16702                      Zimbabwe  \n",
      "16703                      Zimbabwe  \n",
      "\n",
      "[16704 rows x 7 columns]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'MERTRA'  \n",
    "# ARCHIVO   = 'ratio_tasa_actividad_mujer_varon_por_pais_anio.csv'\n",
    "# ya no tiene problemas :D\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'MERTRA'  \n",
    "ARCHIVO   = 'ratio_tasa_actividad_mujer_varon_por_pais_anio.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     anio     pais_o_region  pbi_per_capita_ppa geocodigoFundar  \\\n",
      "0    1950    America Latina         3672.315179            <NA>   \n",
      "1    1951    America Latina         3772.801926            <NA>   \n",
      "2    1952    America Latina         3800.175320            <NA>   \n",
      "3    1953    America Latina         3852.794160            <NA>   \n",
      "4    1954    America Latina         3973.484863            <NA>   \n",
      "..    ...               ...                 ...             ...   \n",
      "214  2018  Tigres Asiaticos        42850.848420            <NA>   \n",
      "215  2019  Tigres Asiaticos        43538.234690            <NA>   \n",
      "216  2020  Tigres Asiaticos        43324.091790            <NA>   \n",
      "217  2021  Tigres Asiaticos        46071.454090            <NA>   \n",
      "218  2022  Tigres Asiaticos        47014.943250            <NA>   \n",
      "\n",
      "    geonombreFundar  \n",
      "0              <NA>  \n",
      "1              <NA>  \n",
      "2              <NA>  \n",
      "3              <NA>  \n",
      "4              <NA>  \n",
      "..              ...  \n",
      "214            <NA>  \n",
      "215            <NA>  \n",
      "216            <NA>  \n",
      "217            <NA>  \n",
      "218            <NA>  \n",
      "\n",
      "[219 rows x 5 columns]\n",
      "None\n",
      "     anio     pais_o_region  pbi_per_capita_ppa geocodigoFundar  \\\n",
      "0    1950    America Latina         3672.315179      DESIGU_AML   \n",
      "1    1951    America Latina         3772.801926      DESIGU_AML   \n",
      "2    1952    America Latina         3800.175320      DESIGU_AML   \n",
      "3    1953    America Latina         3852.794160      DESIGU_AML   \n",
      "4    1954    America Latina         3973.484863      DESIGU_AML   \n",
      "..    ...               ...                 ...             ...   \n",
      "214  2018  Tigres Asiaticos        42850.848420            <NA>   \n",
      "215  2019  Tigres Asiaticos        43538.234690            <NA>   \n",
      "216  2020  Tigres Asiaticos        43324.091790            <NA>   \n",
      "217  2021  Tigres Asiaticos        46071.454090            <NA>   \n",
      "218  2022  Tigres Asiaticos        47014.943250            <NA>   \n",
      "\n",
      "    geonombreFundar  \n",
      "0    América Latina  \n",
      "1    América Latina  \n",
      "2    América Latina  \n",
      "3    América Latina  \n",
      "4    América Latina  \n",
      "..              ...  \n",
      "214            <NA>  \n",
      "215            <NA>  \n",
      "216            <NA>  \n",
      "217            <NA>  \n",
      "218            <NA>  \n",
      "\n",
      "[219 rows x 5 columns]\n",
      "    TOPICO                       csv  \\\n",
      "0   ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "1   ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "2   ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "3   ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "4   ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "..     ...                       ...   \n",
      "68  ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "69  ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "70  ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "71  ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "72  ACECON  9_pibpc_ppa_log_1950.csv   \n",
      "\n",
      "                                             problema  \n",
      "0   Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "1   Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "2   Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "3   Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "4   Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "..                                                ...  \n",
      "68  Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "69  Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "70  Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "71  Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "72  Para la string 'TIGRES ASIATICOS' generar un c...  \n",
      "\n",
      "[73 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'ACECON'  \n",
    "# ARCHIVO   = '9_pibpc_ppa_log_1950.csv'\n",
    "# Para la string 'TIGRES ASIATICOS' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'ACECON'  \n",
    "ARCHIVO   = '9_pibpc_ppa_log_1950.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     region_code                  region_name  year  poverty_line  \\\n",
      "0            AFE  Eastern and Southern Africa  1993          2.15   \n",
      "1            AFE  Eastern and Southern Africa  1994          2.15   \n",
      "2            AFE  Eastern and Southern Africa  1995          2.15   \n",
      "3            AFE  Eastern and Southern Africa  1996          2.15   \n",
      "4            AFE  Eastern and Southern Africa  1998          2.15   \n",
      "...          ...                          ...   ...           ...   \n",
      "1910         WLD                        World  2015         15.00   \n",
      "1911         WLD                        World  2016         15.00   \n",
      "1912         WLD                        World  2017         15.00   \n",
      "1913         WLD                        World  2018         15.00   \n",
      "1914         WLD                        World  2019         15.00   \n",
      "\n",
      "      poverty_rate geocodigoFundar geonombreFundar  \n",
      "0         0.584803            <NA>            <NA>  \n",
      "1         0.589474            <NA>            <NA>  \n",
      "2         0.579662            <NA>            <NA>  \n",
      "3         0.568109            <NA>            <NA>  \n",
      "4         0.569110            <NA>            <NA>  \n",
      "...            ...             ...             ...  \n",
      "1910      0.731318            <NA>            <NA>  \n",
      "1911      0.724939            <NA>            <NA>  \n",
      "1912      0.717605            <NA>            <NA>  \n",
      "1913      0.711307            <NA>            <NA>  \n",
      "1914      0.705482            <NA>            <NA>  \n",
      "\n",
      "[1915 rows x 7 columns]\n",
      "region_code\n",
      "     region_code                  region_name  year  poverty_line  \\\n",
      "0            AFE  Eastern and Southern Africa  1993          2.15   \n",
      "1            AFE  Eastern and Southern Africa  1994          2.15   \n",
      "2            AFE  Eastern and Southern Africa  1995          2.15   \n",
      "3            AFE  Eastern and Southern Africa  1996          2.15   \n",
      "4            AFE  Eastern and Southern Africa  1998          2.15   \n",
      "...          ...                          ...   ...           ...   \n",
      "1910         WLD                        World  2015         15.00   \n",
      "1911         WLD                        World  2016         15.00   \n",
      "1912         WLD                        World  2017         15.00   \n",
      "1913         WLD                        World  2018         15.00   \n",
      "1914         WLD                        World  2019         15.00   \n",
      "\n",
      "      poverty_rate geocodigoFundar               geonombreFundar  \n",
      "0         0.584803             AFE  África Oriental y Meridional  \n",
      "1         0.589474             AFE  África Oriental y Meridional  \n",
      "2         0.579662             AFE  África Oriental y Meridional  \n",
      "3         0.568109             AFE  África Oriental y Meridional  \n",
      "4         0.569110             AFE  África Oriental y Meridional  \n",
      "...            ...             ...                           ...  \n",
      "1910      0.731318             WLD                         Mundo  \n",
      "1911      0.724939             WLD                         Mundo  \n",
      "1912      0.717605             WLD                         Mundo  \n",
      "1913      0.711307             WLD                         Mundo  \n",
      "1914      0.705482             WLD                         Mundo  \n",
      "\n",
      "[1915 rows x 7 columns]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'POBREZ'  \n",
    "# ARCHIVO   = 'ISA_global-poverty_it3.csv'\n",
    "# no se registra ni columna similar a geocodigo ni desc_fundar, a pesar de tener una columna llamada region porque el contenido es ‘GBA' y 'national' \n",
    "# que son georreferencial pero no son parte de desc_fundar, y no tiene una columna de geocodigo tampoco\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'POBREZ'  \n",
    "ARCHIVO   = 'ISA_global-poverty_it3.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year period_type  date        survey    region  poverty_rate  \\\n",
      "0   1988         mes     5   EPH-Puntual       GBA          29.8   \n",
      "1   1988         mes    10   EPH-Puntual       GBA          32.3   \n",
      "2   1989         mes     5   EPH-Puntual       GBA          25.9   \n",
      "3   1989         mes    10   EPH-Puntual       GBA          47.3   \n",
      "4   1990         mes     5   EPH-Puntual       GBA          42.5   \n",
      "5   1990         mes    10   EPH-Puntual       GBA          33.7   \n",
      "6   1991         mes     5   EPH-Puntual       GBA          28.9   \n",
      "7   1991         mes    10   EPH-Puntual       GBA          21.5   \n",
      "8   1992         mes     5   EPH-Puntual       GBA          19.3   \n",
      "9   1992         mes    10   EPH-Puntual       GBA          17.8   \n",
      "10  1993         mes     5   EPH-Puntual       GBA          17.7   \n",
      "11  1993         mes    10   EPH-Puntual       GBA          16.8   \n",
      "12  1994         mes     5   EPH-Puntual       GBA          16.1   \n",
      "13  1994         mes    10   EPH-Puntual       GBA          19.0   \n",
      "14  1995         mes     5   EPH-Puntual       GBA          22.2   \n",
      "15  1995         mes    10   EPH-Puntual       GBA          24.8   \n",
      "16  1996         mes     5   EPH-Puntual       GBA          26.7   \n",
      "17  1996         mes    10   EPH-Puntual       GBA          27.9   \n",
      "18  1997         mes     5   EPH-Puntual       GBA          26.3   \n",
      "19  1997         mes    10   EPH-Puntual       GBA          26.0   \n",
      "20  1998         mes     5   EPH-Puntual       GBA          24.3   \n",
      "21  1998         mes    10   EPH-Puntual       GBA          25.9   \n",
      "22  1999         mes     5   EPH-Puntual       GBA          27.1   \n",
      "23  1999         mes    10   EPH-Puntual       GBA          26.7   \n",
      "24  2000         mes     5   EPH-Puntual       GBA          29.7   \n",
      "25  2000         mes    10   EPH-Puntual       GBA          28.9   \n",
      "26  2001         mes     5   EPH-Puntual       GBA          32.7   \n",
      "27  2001         mes    10   EPH-Puntual       GBA          35.4   \n",
      "28  2002         mes     5   EPH-Puntual       GBA          49.7   \n",
      "29  2002         mes    10   EPH-Puntual       GBA          54.3   \n",
      "30  2003         mes     5   EPH-Puntual       GBA          51.7   \n",
      "31  2003    semestre     1  EPH-Continua       GBA          52.3   \n",
      "32  2003    semestre     2  EPH-Continua       GBA          46.2   \n",
      "33  2004    semestre     1  EPH-Continua       GBA          42.7   \n",
      "34  2004    semestre     2  EPH-Continua       GBA          37.7   \n",
      "35  2005    semestre     1  EPH-Continua       GBA          38.0   \n",
      "36  2005    semestre     2  EPH-Continua       GBA          30.9   \n",
      "37  2006    semestre     1  EPH-Continua       GBA          29.4   \n",
      "38  2006    semestre     2  EPH-Continua       GBA          25.5   \n",
      "39  2001         mes     5   EPH-Puntual  national          35.9   \n",
      "40  2001         mes    10   EPH-Puntual  national          38.3   \n",
      "41  2002         mes     5   EPH-Puntual  national          53.0   \n",
      "42  2002         mes    10   EPH-Puntual  national          57.5   \n",
      "43  2003         mes     5   EPH-Puntual  national          54.7   \n",
      "44  2003    semestre     1  EPH-Continua  national          54.0   \n",
      "45  2003    semestre     2  EPH-Continua  national          47.8   \n",
      "46  2004    semestre     1  EPH-Continua  national          44.3   \n",
      "47  2004    semestre     2  EPH-Continua  national          40.2   \n",
      "48  2005    semestre     1  EPH-Continua  national          38.9   \n",
      "49  2005    semestre     2  EPH-Continua  national          33.8   \n",
      "50  2006    semestre     1  EPH-Continua  national          31.4   \n",
      "51  2006    semestre     2  EPH-Continua  national          26.9   \n",
      "\n",
      "   geocodigoFundar geonombreFundar  \n",
      "0             <NA>            <NA>  \n",
      "1             <NA>            <NA>  \n",
      "2             <NA>            <NA>  \n",
      "3             <NA>            <NA>  \n",
      "4             <NA>            <NA>  \n",
      "5             <NA>            <NA>  \n",
      "6             <NA>            <NA>  \n",
      "7             <NA>            <NA>  \n",
      "8             <NA>            <NA>  \n",
      "9             <NA>            <NA>  \n",
      "10            <NA>            <NA>  \n",
      "11            <NA>            <NA>  \n",
      "12            <NA>            <NA>  \n",
      "13            <NA>            <NA>  \n",
      "14            <NA>            <NA>  \n",
      "15            <NA>            <NA>  \n",
      "16            <NA>            <NA>  \n",
      "17            <NA>            <NA>  \n",
      "18            <NA>            <NA>  \n",
      "19            <NA>            <NA>  \n",
      "20            <NA>            <NA>  \n",
      "21            <NA>            <NA>  \n",
      "22            <NA>            <NA>  \n",
      "23            <NA>            <NA>  \n",
      "24            <NA>            <NA>  \n",
      "25            <NA>            <NA>  \n",
      "26            <NA>            <NA>  \n",
      "27            <NA>            <NA>  \n",
      "28            <NA>            <NA>  \n",
      "29            <NA>            <NA>  \n",
      "30            <NA>            <NA>  \n",
      "31            <NA>            <NA>  \n",
      "32            <NA>            <NA>  \n",
      "33            <NA>            <NA>  \n",
      "34            <NA>            <NA>  \n",
      "35            <NA>            <NA>  \n",
      "36            <NA>            <NA>  \n",
      "37            <NA>            <NA>  \n",
      "38            <NA>            <NA>  \n",
      "39            <NA>            <NA>  \n",
      "40            <NA>            <NA>  \n",
      "41            <NA>            <NA>  \n",
      "42            <NA>            <NA>  \n",
      "43            <NA>            <NA>  \n",
      "44            <NA>            <NA>  \n",
      "45            <NA>            <NA>  \n",
      "46            <NA>            <NA>  \n",
      "47            <NA>            <NA>  \n",
      "48            <NA>            <NA>  \n",
      "49            <NA>            <NA>  \n",
      "50            <NA>            <NA>  \n",
      "51            <NA>            <NA>  \n",
      "None\n",
      "    year period_type  date        survey    region  poverty_rate  \\\n",
      "0   1988         mes     5   EPH-Puntual       GBA          29.8   \n",
      "1   1988         mes    10   EPH-Puntual       GBA          32.3   \n",
      "2   1989         mes     5   EPH-Puntual       GBA          25.9   \n",
      "3   1989         mes    10   EPH-Puntual       GBA          47.3   \n",
      "4   1990         mes     5   EPH-Puntual       GBA          42.5   \n",
      "5   1990         mes    10   EPH-Puntual       GBA          33.7   \n",
      "6   1991         mes     5   EPH-Puntual       GBA          28.9   \n",
      "7   1991         mes    10   EPH-Puntual       GBA          21.5   \n",
      "8   1992         mes     5   EPH-Puntual       GBA          19.3   \n",
      "9   1992         mes    10   EPH-Puntual       GBA          17.8   \n",
      "10  1993         mes     5   EPH-Puntual       GBA          17.7   \n",
      "11  1993         mes    10   EPH-Puntual       GBA          16.8   \n",
      "12  1994         mes     5   EPH-Puntual       GBA          16.1   \n",
      "13  1994         mes    10   EPH-Puntual       GBA          19.0   \n",
      "14  1995         mes     5   EPH-Puntual       GBA          22.2   \n",
      "15  1995         mes    10   EPH-Puntual       GBA          24.8   \n",
      "16  1996         mes     5   EPH-Puntual       GBA          26.7   \n",
      "17  1996         mes    10   EPH-Puntual       GBA          27.9   \n",
      "18  1997         mes     5   EPH-Puntual       GBA          26.3   \n",
      "19  1997         mes    10   EPH-Puntual       GBA          26.0   \n",
      "20  1998         mes     5   EPH-Puntual       GBA          24.3   \n",
      "21  1998         mes    10   EPH-Puntual       GBA          25.9   \n",
      "22  1999         mes     5   EPH-Puntual       GBA          27.1   \n",
      "23  1999         mes    10   EPH-Puntual       GBA          26.7   \n",
      "24  2000         mes     5   EPH-Puntual       GBA          29.7   \n",
      "25  2000         mes    10   EPH-Puntual       GBA          28.9   \n",
      "26  2001         mes     5   EPH-Puntual       GBA          32.7   \n",
      "27  2001         mes    10   EPH-Puntual       GBA          35.4   \n",
      "28  2002         mes     5   EPH-Puntual       GBA          49.7   \n",
      "29  2002         mes    10   EPH-Puntual       GBA          54.3   \n",
      "30  2003         mes     5   EPH-Puntual       GBA          51.7   \n",
      "31  2003    semestre     1  EPH-Continua       GBA          52.3   \n",
      "32  2003    semestre     2  EPH-Continua       GBA          46.2   \n",
      "33  2004    semestre     1  EPH-Continua       GBA          42.7   \n",
      "34  2004    semestre     2  EPH-Continua       GBA          37.7   \n",
      "35  2005    semestre     1  EPH-Continua       GBA          38.0   \n",
      "36  2005    semestre     2  EPH-Continua       GBA          30.9   \n",
      "37  2006    semestre     1  EPH-Continua       GBA          29.4   \n",
      "38  2006    semestre     2  EPH-Continua       GBA          25.5   \n",
      "39  2001         mes     5   EPH-Puntual  national          35.9   \n",
      "40  2001         mes    10   EPH-Puntual  national          38.3   \n",
      "41  2002         mes     5   EPH-Puntual  national          53.0   \n",
      "42  2002         mes    10   EPH-Puntual  national          57.5   \n",
      "43  2003         mes     5   EPH-Puntual  national          54.7   \n",
      "44  2003    semestre     1  EPH-Continua  national          54.0   \n",
      "45  2003    semestre     2  EPH-Continua  national          47.8   \n",
      "46  2004    semestre     1  EPH-Continua  national          44.3   \n",
      "47  2004    semestre     2  EPH-Continua  national          40.2   \n",
      "48  2005    semestre     1  EPH-Continua  national          38.9   \n",
      "49  2005    semestre     2  EPH-Continua  national          33.8   \n",
      "50  2006    semestre     1  EPH-Continua  national          31.4   \n",
      "51  2006    semestre     2  EPH-Continua  national          26.9   \n",
      "\n",
      "   geocodigoFundar geonombreFundar  \n",
      "0             <NA>            <NA>  \n",
      "1             <NA>            <NA>  \n",
      "2             <NA>            <NA>  \n",
      "3             <NA>            <NA>  \n",
      "4             <NA>            <NA>  \n",
      "5             <NA>            <NA>  \n",
      "6             <NA>            <NA>  \n",
      "7             <NA>            <NA>  \n",
      "8             <NA>            <NA>  \n",
      "9             <NA>            <NA>  \n",
      "10            <NA>            <NA>  \n",
      "11            <NA>            <NA>  \n",
      "12            <NA>            <NA>  \n",
      "13            <NA>            <NA>  \n",
      "14            <NA>            <NA>  \n",
      "15            <NA>            <NA>  \n",
      "16            <NA>            <NA>  \n",
      "17            <NA>            <NA>  \n",
      "18            <NA>            <NA>  \n",
      "19            <NA>            <NA>  \n",
      "20            <NA>            <NA>  \n",
      "21            <NA>            <NA>  \n",
      "22            <NA>            <NA>  \n",
      "23            <NA>            <NA>  \n",
      "24            <NA>            <NA>  \n",
      "25            <NA>            <NA>  \n",
      "26            <NA>            <NA>  \n",
      "27            <NA>            <NA>  \n",
      "28            <NA>            <NA>  \n",
      "29            <NA>            <NA>  \n",
      "30            <NA>            <NA>  \n",
      "31            <NA>            <NA>  \n",
      "32            <NA>            <NA>  \n",
      "33            <NA>            <NA>  \n",
      "34            <NA>            <NA>  \n",
      "35            <NA>            <NA>  \n",
      "36            <NA>            <NA>  \n",
      "37            <NA>            <NA>  \n",
      "38            <NA>            <NA>  \n",
      "39            <NA>            <NA>  \n",
      "40            <NA>            <NA>  \n",
      "41            <NA>            <NA>  \n",
      "42            <NA>            <NA>  \n",
      "43            <NA>            <NA>  \n",
      "44            <NA>            <NA>  \n",
      "45            <NA>            <NA>  \n",
      "46            <NA>            <NA>  \n",
      "47            <NA>            <NA>  \n",
      "48            <NA>            <NA>  \n",
      "49            <NA>            <NA>  \n",
      "50            <NA>            <NA>  \n",
      "51            <NA>            <NA>  \n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'POBREZ'  \n",
    "# ARCHIVO   = 'ISA_pobreza_monetaria_it2.csv'\n",
    "# me queda ver\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'POBREZ'  \n",
    "ARCHIVO   = 'ISA_pobreza_monetaria_it2.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    anio  mediana_paises_inflacion   pais_o_grupo_de_paises geocodigoFundar  \\\n",
      "0   1992                 17.545874                Argentina            <NA>   \n",
      "1   1993                  7.364981                Argentina            <NA>   \n",
      "2   1994                  3.854381                Argentina            <NA>   \n",
      "3   1995                  1.607754                Argentina            <NA>   \n",
      "4   1996                  0.054311                Argentina            <NA>   \n",
      "..   ...                       ...                      ...             ...   \n",
      "59  2019                  3.531500  América Latina y Caribe            <NA>   \n",
      "60  2020                  3.040500  América Latina y Caribe            <NA>   \n",
      "61  2021                  6.982000  América Latina y Caribe            <NA>   \n",
      "62  2022                  8.375500  América Latina y Caribe            <NA>   \n",
      "63  2023                  4.299000  América Latina y Caribe            <NA>   \n",
      "\n",
      "   geonombreFundar  \n",
      "0             <NA>  \n",
      "1             <NA>  \n",
      "2             <NA>  \n",
      "3             <NA>  \n",
      "4             <NA>  \n",
      "..             ...  \n",
      "59            <NA>  \n",
      "60            <NA>  \n",
      "61            <NA>  \n",
      "62            <NA>  \n",
      "63            <NA>  \n",
      "\n",
      "[64 rows x 5 columns]\n",
      "None\n",
      "    anio  mediana_paises_inflacion   pais_o_grupo_de_paises geocodigoFundar  \\\n",
      "0   1992                 17.545874                Argentina            <NA>   \n",
      "1   1993                  7.364981                Argentina            <NA>   \n",
      "2   1994                  3.854381                Argentina            <NA>   \n",
      "3   1995                  1.607754                Argentina            <NA>   \n",
      "4   1996                  0.054311                Argentina            <NA>   \n",
      "..   ...                       ...                      ...             ...   \n",
      "59  2019                  3.531500  América Latina y Caribe            <NA>   \n",
      "60  2020                  3.040500  América Latina y Caribe            <NA>   \n",
      "61  2021                  6.982000  América Latina y Caribe            <NA>   \n",
      "62  2022                  8.375500  América Latina y Caribe            <NA>   \n",
      "63  2023                  4.299000  América Latina y Caribe            <NA>   \n",
      "\n",
      "   geonombreFundar  \n",
      "0             <NA>  \n",
      "1             <NA>  \n",
      "2             <NA>  \n",
      "3             <NA>  \n",
      "4             <NA>  \n",
      "..             ...  \n",
      "59            <NA>  \n",
      "60            <NA>  \n",
      "61            <NA>  \n",
      "62            <NA>  \n",
      "63            <NA>  \n",
      "\n",
      "[64 rows x 5 columns]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'PRECIO'  \n",
    "# ARCHIVO   = '7_comparacion_inflacion_mediana_argentina_latam_1992_2022.csv'\n",
    "# No se cumple condición X% Los valores de esta columna son: 'Argentina', 'América Latina y Caribe' \n",
    "# 'América Latina y Caribe' no aparece en el geonomenclador, si aparece ‘América Latina y el Caribe’. \n",
    "# No se lee columna similar ni a geocodigo ni a desc_fundar\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'PRECIO'  \n",
    "ARCHIVO   = '7_comparacion_inflacion_mediana_argentina_latam_1992_2022.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              region  tipo_energia  valor_en_mw  porcentaje  \\\n",
      "0   CABA y Provincia de Buenos Aires    Bioenergía           48    3.219316   \n",
      "1   CABA y Provincia de Buenos Aires        Eólica         1443   96.780684   \n",
      "2   CABA y Provincia de Buenos Aires  Fotovoltaica            0    0.000000   \n",
      "3   CABA y Provincia de Buenos Aires         Hidro            0    0.000000   \n",
      "4                             Centro    Bioenergía           34    6.679764   \n",
      "5                             Centro        Eólica          240   47.151277   \n",
      "6                             Centro  Fotovoltaica          118   23.182711   \n",
      "7                             Centro         Hidro          117   22.986248   \n",
      "8                            Comahue    Bioenergía            2    0.680272   \n",
      "9                            Comahue        Eólica          253   86.054422   \n",
      "10                           Comahue  Fotovoltaica            7    2.380952   \n",
      "11                           Comahue         Hidro           32   10.884354   \n",
      "12                              Cuyo    Bioenergía            0    0.000000   \n",
      "13                              Cuyo        Eólica            0    0.000000   \n",
      "14                              Cuyo  Fotovoltaica          564   75.300401   \n",
      "15                              Cuyo         Hidro          185   24.699599   \n",
      "16                           Litoral    Bioenergía           12   85.714286   \n",
      "17                           Litoral        Eólica            0    0.000000   \n",
      "18                           Litoral  Fotovoltaica            0    0.000000   \n",
      "19                           Litoral         Hidro            2   14.285714   \n",
      "20                 Noreste Argentino    Bioenergía          109  100.000000   \n",
      "21                 Noreste Argentino        Eólica            0    0.000000   \n",
      "22                 Noreste Argentino  Fotovoltaica            0    0.000000   \n",
      "23                 Noreste Argentino         Hidro            0    0.000000   \n",
      "24                Noroeste Argentino    Bioenergía           72    6.190886   \n",
      "25                Noroeste Argentino        Eólica          194   16.680997   \n",
      "26                Noroeste Argentino  Fotovoltaica          778   66.895959   \n",
      "27                Noroeste Argentino         Hidro          119   10.232158   \n",
      "28                         Patagonia    Bioenergía            0    0.000000   \n",
      "29                         Patagonia        Eólica         1576   97.104128   \n",
      "30                         Patagonia  Fotovoltaica            0    0.000000   \n",
      "31                         Patagonia         Hidro           47    2.895872   \n",
      "32                             Total    Bioenergía          277    4.653898   \n",
      "33                             Total        Eólica         3706   62.264785   \n",
      "34                             Total  Fotovoltaica         1467   24.647177   \n",
      "35                             Total         Hidro          502    8.434140   \n",
      "\n",
      "   geocodigoFundar geonombreFundar  \n",
      "0             <NA>            <NA>  \n",
      "1             <NA>            <NA>  \n",
      "2             <NA>            <NA>  \n",
      "3             <NA>            <NA>  \n",
      "4             <NA>            <NA>  \n",
      "5             <NA>            <NA>  \n",
      "6             <NA>            <NA>  \n",
      "7             <NA>            <NA>  \n",
      "8             <NA>            <NA>  \n",
      "9             <NA>            <NA>  \n",
      "10            <NA>            <NA>  \n",
      "11            <NA>            <NA>  \n",
      "12            <NA>            <NA>  \n",
      "13            <NA>            <NA>  \n",
      "14            <NA>            <NA>  \n",
      "15            <NA>            <NA>  \n",
      "16            <NA>            <NA>  \n",
      "17            <NA>            <NA>  \n",
      "18            <NA>            <NA>  \n",
      "19            <NA>            <NA>  \n",
      "20            <NA>            <NA>  \n",
      "21            <NA>            <NA>  \n",
      "22            <NA>            <NA>  \n",
      "23            <NA>            <NA>  \n",
      "24            <NA>            <NA>  \n",
      "25            <NA>            <NA>  \n",
      "26            <NA>            <NA>  \n",
      "27            <NA>            <NA>  \n",
      "28            <NA>            <NA>  \n",
      "29            <NA>            <NA>  \n",
      "30            <NA>            <NA>  \n",
      "31            <NA>            <NA>  \n",
      "32            <NA>            <NA>  \n",
      "33            <NA>            <NA>  \n",
      "34            <NA>            <NA>  \n",
      "35            <NA>            <NA>  \n",
      "None\n",
      "                              region  tipo_energia  valor_en_mw  porcentaje  \\\n",
      "0   CABA y Provincia de Buenos Aires    Bioenergía           48    3.219316   \n",
      "1   CABA y Provincia de Buenos Aires        Eólica         1443   96.780684   \n",
      "2   CABA y Provincia de Buenos Aires  Fotovoltaica            0    0.000000   \n",
      "3   CABA y Provincia de Buenos Aires         Hidro            0    0.000000   \n",
      "4                             Centro    Bioenergía           34    6.679764   \n",
      "5                             Centro        Eólica          240   47.151277   \n",
      "6                             Centro  Fotovoltaica          118   23.182711   \n",
      "7                             Centro         Hidro          117   22.986248   \n",
      "8                            Comahue    Bioenergía            2    0.680272   \n",
      "9                            Comahue        Eólica          253   86.054422   \n",
      "10                           Comahue  Fotovoltaica            7    2.380952   \n",
      "11                           Comahue         Hidro           32   10.884354   \n",
      "12                              Cuyo    Bioenergía            0    0.000000   \n",
      "13                              Cuyo        Eólica            0    0.000000   \n",
      "14                              Cuyo  Fotovoltaica          564   75.300401   \n",
      "15                              Cuyo         Hidro          185   24.699599   \n",
      "16                           Litoral    Bioenergía           12   85.714286   \n",
      "17                           Litoral        Eólica            0    0.000000   \n",
      "18                           Litoral  Fotovoltaica            0    0.000000   \n",
      "19                           Litoral         Hidro            2   14.285714   \n",
      "20                 Noreste Argentino    Bioenergía          109  100.000000   \n",
      "21                 Noreste Argentino        Eólica            0    0.000000   \n",
      "22                 Noreste Argentino  Fotovoltaica            0    0.000000   \n",
      "23                 Noreste Argentino         Hidro            0    0.000000   \n",
      "24                Noroeste Argentino    Bioenergía           72    6.190886   \n",
      "25                Noroeste Argentino        Eólica          194   16.680997   \n",
      "26                Noroeste Argentino  Fotovoltaica          778   66.895959   \n",
      "27                Noroeste Argentino         Hidro          119   10.232158   \n",
      "28                         Patagonia    Bioenergía            0    0.000000   \n",
      "29                         Patagonia        Eólica         1576   97.104128   \n",
      "30                         Patagonia  Fotovoltaica            0    0.000000   \n",
      "31                         Patagonia         Hidro           47    2.895872   \n",
      "32                             Total    Bioenergía          277    4.653898   \n",
      "33                             Total        Eólica         3706   62.264785   \n",
      "34                             Total  Fotovoltaica         1467   24.647177   \n",
      "35                             Total         Hidro          502    8.434140   \n",
      "\n",
      "   geocodigoFundar geonombreFundar  \n",
      "0             <NA>            <NA>  \n",
      "1             <NA>            <NA>  \n",
      "2             <NA>            <NA>  \n",
      "3             <NA>            <NA>  \n",
      "4             <NA>            <NA>  \n",
      "5             <NA>            <NA>  \n",
      "6             <NA>            <NA>  \n",
      "7             <NA>            <NA>  \n",
      "8             <NA>            <NA>  \n",
      "9             <NA>            <NA>  \n",
      "10            <NA>            <NA>  \n",
      "11            <NA>            <NA>  \n",
      "12            <NA>            <NA>  \n",
      "13            <NA>            <NA>  \n",
      "14            <NA>            <NA>  \n",
      "15            <NA>            <NA>  \n",
      "16            <NA>            <NA>  \n",
      "17            <NA>            <NA>  \n",
      "18            <NA>            <NA>  \n",
      "19            <NA>            <NA>  \n",
      "20            <NA>            <NA>  \n",
      "21            <NA>            <NA>  \n",
      "22            <NA>            <NA>  \n",
      "23            <NA>            <NA>  \n",
      "24            <NA>            <NA>  \n",
      "25            <NA>            <NA>  \n",
      "26            <NA>            <NA>  \n",
      "27            <NA>            <NA>  \n",
      "28            <NA>            <NA>  \n",
      "29            <NA>            <NA>  \n",
      "30            <NA>            <NA>  \n",
      "31            <NA>            <NA>  \n",
      "32            <NA>            <NA>  \n",
      "33            <NA>            <NA>  \n",
      "34            <NA>            <NA>  \n",
      "35            <NA>            <NA>  \n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# PRUEBA SOBRE OTRO CSV DE UN TOPICO ARGENDATA CON PROBLEMA: \n",
    "# TOPICO    = 'TRANEN'  \n",
    "# ARCHIVO   = 'potencia_instalada_renov_regional.csv'\n",
    "# No se cumple condición X% Los valores de esta columna son: 'CABA y Provincia de Buenos Aires', 'Centro', 'Comahue',\n",
    "# 'Cuyo', 'Litoral', 'Noreste Argentino', 'Noroeste Argentino', 'Patagonia', 'Total'\n",
    "# 'CABA y Provincia de Buenos Aires',  'Noreste Argentino', 'Noroeste Argentino' no aparece en el geonomenclador.\n",
    "# No se lee columna similar ni a geocodigo ni a desc_fundar\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# funcion eliminar tildes\n",
    "def sin_tildes(s):\n",
    "    nkfd = unicodedata.normalize('NFKD', s)\n",
    "    return ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
    "\n",
    "# parametros: especificar el tópico y el csv de ese topico\n",
    "TOPICO    = 'TRANEN'  \n",
    "ARCHIVO   = 'potencia_instalada_renov_regional.csv'\n",
    "\n",
    "# rutas\n",
    "base_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata'\n",
    "dest_dir = '/home/capuccino/Desktop/TrabajoFundar/Argendata_renaming/data/data_argendata_new'\n",
    "\n",
    "# df inputs\n",
    "df_cod = df_codificacion\n",
    "df_geo = df_geonomenclador\n",
    "df_cols = columnscsv_Geocodigo_descFundar  \n",
    "\n",
    "# dicc de name_long y desc_fundar por geocodigo del geonomenclador argendata\n",
    "geonames_by_geocode = {}\n",
    "for _, row in df_geo.iterrows():\n",
    "    code = sin_tildes(str(row['geocodigo']).strip().upper())\n",
    "    geonames_by_geocode[code] = (row['name_long'], row['desc_fundar'])\n",
    "\n",
    "# abro el csv con respectivos encoding y delimiter, cfg de configuracion \n",
    "cfg = df_cod[(df_cod['TOPICO']==TOPICO) & \n",
    "             (df_cod['archivo_csv']==ARCHIVO)]\n",
    "if cfg.empty:\n",
    "    print(f'No se encontro info de encoding/delimiter para {TOPICO}/{ARCHIVO}')\n",
    "encoding   = cfg.iloc[0]['encoding']\n",
    "delimiter = cfg.iloc[0]['delimiter']\n",
    "\n",
    "src_path = os.path.join(base_dir, TOPICO, ARCHIVO)\n",
    "df = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) #csv de interes como un dartaframe\n",
    "df_original = pd.read_csv(src_path, encoding = encoding, delimiter = delimiter) \n",
    "\n",
    "# print(df)\n",
    "\n",
    "# le sumo al csv las nuevas columnas de interes\n",
    "df['geocodigoFundar'] = pd.NA\n",
    "df['geonombreFundar'] = pd.NA\n",
    "\n",
    "print(df)\n",
    "\n",
    "# del df columnscsv_Geocodigo_descFundar toma la info del csv y topico que me interesa\n",
    "row_cols = df_cols[(df_cols['TOPICO'] == TOPICO) & (df_cols['archivo_csv'] == ARCHIVO)]\n",
    "col_code = row_cols.iloc[0]['columna_Geocodigo']   if not row_cols.empty else None\n",
    "col_desc = row_cols.iloc[0]['columna_DescFundar']  if not row_cols.empty else None\n",
    "\n",
    "print(col_code)\n",
    "\n",
    "# lista para tomar nota de los \"problemas\"\n",
    "problems = []\n",
    "\n",
    "# recorro cada fila del \n",
    "for idx in df.index:\n",
    "    # CASO A: existe columna correspondiente a geocodigo del geonomenclador en el csv\n",
    "\n",
    "    # A.0) hay un codigo presente en geocodigos, lo relleno con name_long o desc_fundar\n",
    "    if pd.notna(col_code):\n",
    "        raw_code = str(df.at[idx, col_code])\n",
    "        norm_code = sin_tildes(raw_code.strip().upper())\n",
    "        if norm_code in geonames_by_geocode:\n",
    "            nl, desc0 = geonames_by_geocode[norm_code]\n",
    "            df.at[idx, 'geocodigoFundar'] = norm_code\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0 # relleno con name_long si existe, sino relleno con la sting de desc_fundar\n",
    "        else:\n",
    "            # A.1) hay un codigo NO presente en geocodigos, pero intento por columna desc_fundar\n",
    "            if pd.notna(col_desc):\n",
    "                raw_desc = str(df.at[idx, col_desc])\n",
    "                norm_desc = sin_tildes(raw_desc.strip().upper()) # normalizo columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%\n",
    "                # bsqueda de coincidencias en desc_fundar\n",
    "                matched_code = []\n",
    "                for code_key, (_nl, desc0) in geonames_by_geocode.items(): # _nl para hacer explicito que no uso nl en este caso\n",
    "                    norm_desc0 = sin_tildes(str(desc0).strip().upper()) # normalizo desc_fundar del geonomenclador \n",
    "                    #(para comparacion con la columna del csv de interes columna del csv de interes que se corresponderia a desc_fundar segun condicion del X%)\n",
    "                    if norm_desc0 == norm_desc:\n",
    "                        matched_code.append(code_key)\n",
    "                if len(matched_code) == 1:\n",
    "                    c0 = matched_code[0]\n",
    "                    nl, desc0 = geonames_by_geocode[c0]\n",
    "                    df.at[idx, 'geocodigoFundar'] = c0\n",
    "                    df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}', que NO esta incluido en los geocodigos pero tiene la string '{norm_desc}' que matchea sin ambiguedad en desc_fundar.\"\n",
    "                                     })\n",
    "                else:\n",
    "                    problems.append({\n",
    "                        'TOPICO': TOPICO,\n",
    "                        'csv': ARCHIVO,\n",
    "                        'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y tiene la string '{norm_desc}' que NO matchea sin ambiguedad en desc_fundar. Se debe generar un código nuevo (a revisión para ser desambiguado con posibles matches).\"\n",
    "                                    })\n",
    "                    \n",
    "            # A.2) no hay columna desc_fundar para intentar\n",
    "            else:\n",
    "                problems.append({\n",
    "                    'TOPICO': TOPICO,\n",
    "                    'csv': ARCHIVO,\n",
    "                    'problema': f\"dataset tiene el codigo '{norm_code}' que NO esta incluido en los geocodigos y no existe columna desc_fundar. A revisar qué se hace.\" \n",
    "                                })\n",
    "\n",
    "    # Caso B: no existe columna de geocódigo, pero si se tiene de desc_fundar\n",
    "    elif pd.notna(col_desc):\n",
    "        raw_desc = str(df.at[idx, col_desc])\n",
    "        norm_desc = sin_tildes(raw_desc.strip().upper())\n",
    "        # busqueda coincidencias en desc_fundar\n",
    "        matched_code = []\n",
    "        for code_key, (_nl, desc0) in geonames_by_geocode.items():\n",
    "            norm_desc0 = sin_tildes(str(desc0).strip().upper())\n",
    "            if norm_desc0 == norm_desc:\n",
    "                matched_code.append(code_key)\n",
    "        if len(matched_code) == 1:\n",
    "            c0 = matched_code[0]\n",
    "            nl, desc0 = geonames_by_geocode[c0]\n",
    "            df.at[idx, 'geocodigoFundar'] = c0\n",
    "            df.at[idx, 'geonombreFundar'] = nl if pd.notna(nl) else desc0\n",
    "        else:\n",
    "            problems.append({\n",
    "                'TOPICO': TOPICO,\n",
    "                'csv': ARCHIVO,\n",
    "                'problema': f\"Para la string '{norm_desc}' generar un codigo nuevo (a revison para ser desambiguado con posibles matches)\"\n",
    "                            })\n",
    "\n",
    "    # Caso C: no hay columnas correspondientes a geocodigo ni de desc_fundar, entonces dejamos NaN\n",
    "\n",
    "# gurdo csv modificado\n",
    "os.makedirs(os.path.join(dest_dir, TOPICO), exist_ok = True)\n",
    "dst_path = os.path.join(dest_dir, TOPICO, ARCHIVO)\n",
    "df.to_csv(dst_path, index = False)\n",
    "print(df)\n",
    "\n",
    "# genero dataframe de problemas\n",
    "df_problemas = pd.DataFrame(problems)\n",
    "print(df_problemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
